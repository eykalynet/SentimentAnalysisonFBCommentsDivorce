{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyPoL1e9R+Crm3HYKR89kFkb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installs and Imports"],"metadata":{"id":"vetq3pPkZO85"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkxRNCLNYnzZ"},"outputs":[],"source":["!pip install transformers pandas scikit-learn torch matplotlib seaborn wordcloud gensim transformers[torch]"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import XLMRobertaForSequenceClassification, AutoTokenizer, Trainer\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from gensim import corpora\n","from gensim.models import LdaModel\n","import gensim\n","from collections import Counter\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"],"metadata":{"id":"1WyA4y14a_s1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Drive Mounting"],"metadata":{"id":"phR81Sh1bCag"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"XAYCub9BbDp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Loading"],"metadata":{"id":"PoFDt_3ybdmT"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"],"metadata":{"id":"Wc0RIPAbbFUs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_nd = XLMRobertaForSequenceClassification.from_pretrained('/content/drive/My Drive/Research/SentimentAnalysisDivorce/Models/ND_final')\n","model_dn = XLMRobertaForSequenceClassification.from_pretrained('/content/drive/My Drive/Research/SentimentAnalysisDivorce/Models/DN_final')"],"metadata":{"id":"QFD8zfkdbm8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotated_and_pseudolabeled_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/ANNOTATED_AND_PSEUDOLABELED_DATA_01.xlsx'\n","cleaned_preprocessed_path = '/content/drive/My Drive/Research/SentimentAnalysisDivorce/Dataset/CLEANED_PREPROCESSED_DATA_05.xlsx'"],"metadata":{"id":"iQPDtRrnbm7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_annotated_nd = pd.read_excel(annotated_and_pseudolabeled_path, sheet_name='ND')\n","df_annotated_dn = pd.read_excel(annotated_and_pseudolabeled_path, sheet_name='DN')\n","df_cleaned_nd = pd.read_excel(cleaned_preprocessed_path, sheet_name='ND')\n","df_cleaned_dn = pd.read_excel(cleaned_preprocessed_path, sheet_name='DN')"],"metadata":{"id":"UL9gLk9bbwTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_unlabeled_nd = df_cleaned_nd[~df_cleaned_nd['text'].isin(df_annotated_nd['text'])]\n","df_unlabeled_dn = df_cleaned_dn[~df_cleaned_dn['text'].isin(df_annotated_dn['text'])]"],"metadata":{"id":"bes4mlfPc4BA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sentiment Analysis"],"metadata":{"id":"kGtdQLjPc_v3"}},{"cell_type":"code","source":["def preprocess_data(df, tokenizer, max_length=512):\n","    texts = df['text'].tolist()\n","    inputs = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","    return inputs"],"metadata":{"id":"x3dwm81Hc3_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs_unlabeled_nd = preprocess_data(df_unlabeled_nd, tokenizer)\n","inputs_unlabeled_dn = preprocess_data(df_unlabeled_dn, tokenizer)"],"metadata":{"id":"abCWTU0ufPUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs):\n","        self.inputs = inputs\n","\n","    def __len__(self):\n","        return len(self.inputs['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.inputs.items()}\n","        return item"],"metadata":{"id":"-7gIuznKfPTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_unlabeled_nd = CustomDataset(inputs_unlabeled_nd)\n","dataset_unlabeled_dn = CustomDataset(inputs_unlabeled_dn)"],"metadata":{"id":"rKxVTCIZfSXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_sentiments(model, dataset):\n","    trainer = Trainer(model=model)\n","    predictions = trainer.predict(dataset)\n","    preds = np.argmax(predictions.predictions, axis=1)\n","    return preds"],"metadata":{"id":"Yh33I3YLfT2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_unlabeled_nd = predict_sentiments(model_nd, dataset_unlabeled_nd)\n","preds_unlabeled_dn = predict_sentiments(model_dn, dataset_unlabeled_dn)"],"metadata":{"id":"XTBJjcBXfV1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_unlabeled_nd['predicted_label'] = preds_unlabeled_nd\n","df_unlabeled_dn['predicted_label'] = preds_unlabeled_dn"],"metadata":{"id":"C1bZnmSGfXJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_annotated_nd['source'] = 'annotated'\n","df_unlabeled_nd['source'] = 'unlabeled'\n","df_combined_nd = pd.concat([df_annotated_nd, df_unlabeled_nd])"],"metadata":{"id":"hH8TXNwPnAGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_annotated_dn['source'] = 'annotated'\n","df_unlabeled_dn['source'] = 'unlabeled'\n","df_combined_dn = pd.concat([df_annotated_dn, df_unlabeled_dn])"],"metadata":{"id":"OAgYX-mbnA0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_mapping = {\n","    0: 'Very Positive',\n","    1: 'Positive',\n","    2: 'Slightly Positive',\n","    3: 'Neutral',\n","    4: 'Slightly Negative',\n","    5: 'Negative',\n","    6: 'Very Negative'\n","}"],"metadata":{"id":"7ASbiPnYuUXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_combined_nd['predicted_label'] = df_combined_nd['predicted_label'].map(label_mapping)\n","df_combined_dn['predicted_label'] = df_combined_dn['predicted_label'].map(label_mapping)"],"metadata":{"id":"m_EEZcgzuVGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stopword Removal"],"metadata":{"id":"KuPbLVeOCM2h"}},{"cell_type":"code","source":["stop_words = set([\n","    'akin', 'aking', 'ako', 'alin', 'am', 'amin', 'aming', 'ang', 'ano', 'anumang', 'apat', 'at', 'atin', 'ating', 'ay',\n","    'bababa', 'bago', 'bakit', 'bawat', 'bilang', 'dahil', 'dalawa', 'dapat', 'din', 'dito', 'doon', 'gagawin',\n","    'gayunman', 'ginagawa', 'ginawa', 'ginawang', 'gumawa', 'gusto', 'habang', 'hanggang', 'hindi', 'huwag', 'iba',\n","    'ibaba', 'ibabaw', 'ibig', 'ikaw', 'ilagay', 'ilalim', 'ilan', 'inyong', 'isa', 'isang', 'itaas', 'ito', 'iyo',\n","    'iyon', 'iyong', 'ka', 'kahit', 'kailangan', 'kailanman', 'kami', 'kanila', 'kanilang', 'kanino', 'kanya', 'kanyang',\n","    'kapag', 'kapwa', 'karamihan', 'katiyakan', 'katulad', 'kaya', 'kaysa', 'ko', 'kong', 'kulang', 'kumuha', 'kung',\n","    'laban', 'lahat', 'lamang', 'likod', 'lima', 'maaari', 'maaaring', 'maging', 'mahusay', 'makita', 'marami', 'marapat',\n","    'masyado', 'may', 'mayroon', 'mga', 'minsan', 'mismo', 'mula', 'muli', 'na', 'nabanggit', 'naging', 'nagkaroon',\n","    'nais', 'nakita', 'namin', 'napaka', 'narito', 'nasaan', 'ng', 'ngayon', 'ni', 'nila', 'nilang', 'nito', 'niya',\n","    'niyang', 'noon', 'o', 'pa', 'paano', 'pababa', 'paggawa', 'pagitan', 'pagkakaroon', 'pagkatapos', 'palabas',\n","    'pamamagitan', 'panahon', 'pangalawa', 'para', 'paraan', 'pareho', 'pataas', 'pero', 'pumunta', 'pumupunta', 'sa',\n","    'saan', 'sabi', 'sabihin', 'sarili', 'sila', 'sino', 'siya', 'tatlo', 'tayo', 'tulad', 'tungkol', 'una', 'walang',\n","    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n","    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n","    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n","    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n","    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n","    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n","    'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n","    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some'\n","])"],"metadata":{"id":"bZRdZe71CONT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_stop_words(text):\n","    return \" \".join([word for word in text.split() if word.lower() not in stop_words])"],"metadata":{"id":"O1bs-S29CnB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_combined_nd['text'] = df_combined_nd['text'].apply(remove_stop_words)\n","df_combined_dn['text'] = df_combined_dn['text'].apply(remove_stop_words)"],"metadata":{"id":"s3gXGhUACnXm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Visualization"],"metadata":{"id":"FW5GgH3qfY7H"}},{"cell_type":"code","source":["label_order = ['Very Positive', 'Positive', 'Slightly Positive', 'Neutral', 'Slightly Negative', 'Negative', 'Very Negative']"],"metadata":{"id":"U8SPKJHoE30H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Distribution of Sentiments"],"metadata":{"id":"t0VULw-sosWY"}},{"cell_type":"code","source":["def plot_label_distribution_bar(df, title):\n","    df = df.reset_index(drop=True)  # Reset index to avoid duplicate index issues\n","    plt.figure(figsize=(10, 6))\n","    sns.countplot(x='predicted_label', data=df, order=label_order, color='skyblue')\n","    plt.title(title)\n","    plt.xlabel('Predicted Sentiment')\n","    plt.ylabel('Count')\n","    plt.show()"],"metadata":{"id":"It78Av0enDI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_label_distribution_bar(df_combined_nd, 'Sentiment Distribution for Comments on ND (Neutral Survey Posts)')\n","plot_label_distribution_bar(df_combined_dn, 'Sentiment Distribution for Comments on DN (Divorce News Posts)')"],"metadata":{"id":"o96uD1YsnDH8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Proportion of Sentiments"],"metadata":{"id":"RO5MIt4ozPVc"}},{"cell_type":"code","source":["def plot_label_proportion_bar(df, title):\n","    df = df.reset_index(drop=True)  # Reset index to avoid duplicate index issues\n","    plt.figure(figsize=(10, 6))\n","    df['predicted_label'].value_counts(normalize=True).reindex(label_order).plot(kind='bar', color='skyblue')\n","    plt.title(title)\n","    plt.xlabel('Predicted Sentiment')\n","    plt.ylabel('Proportion')\n","    plt.show()"],"metadata":{"id":"IwZ3A4dRzSTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_label_proportion_bar(df_combined_nd, 'Sentiment Proportion for Comments on ND (Neutral Survey Posts)')\n","plot_label_proportion_bar(df_combined_dn, 'Sentiment Proportion for Comments on DN (Divorce News Posts)')"],"metadata":{"id":"gYFSk1PlzUDa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word Clouds"],"metadata":{"id":"eALT_SGNzWkO"}},{"cell_type":"code","source":["def generate_wordcloud(df, label, title):\n","    df = df.reset_index(drop=True)  # Reset index to avoid duplicate index issues\n","    text = \" \".join(comment for comment in df[df['predicted_label'] == label]['text'])\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n","    plt.figure(figsize=(10, 6))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.title(title)\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"zH2OQqftzXsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label in label_order:\n","    generate_wordcloud(df_combined_nd, label, f'Word Cloud for ND - Sentiment {label}')\n","\n","for label in label_order:\n","    generate_wordcloud(df_combined_dn, label, f'Word Cloud for DN - Sentiment {label}')"],"metadata":{"id":"9A87YUtnzZXG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Words Used Per Category"],"metadata":{"id":"SG4GJ91Qzh7U"}},{"cell_type":"code","source":["def most_frequent_words(df, label, top_n=10):\n","    df = df.reset_index(drop=True)  # Reset index to avoid duplicate index issues\n","    text = \" \".join(comment for comment in df[df['predicted_label'] == label]['text'])\n","    words = text.split()\n","    word_freq = pd.Series(words).value_counts().head(top_n)\n","    print(f'Most Frequent Words for Sentiment {label}')\n","    print(word_freq)"],"metadata":{"id":"eWN0-Fw4zjx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label in label_order:\n","    most_frequent_words(df_combined_nd, label)"],"metadata":{"id":"9iUOjdf4zmBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label in label_order:\n","    most_frequent_words(df_combined_dn, label)"],"metadata":{"id":"FrMRsL70znWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"chiMBVxZzPUM"}},{"cell_type":"markdown","source":["# Topic Modeling"],"metadata":{"id":"J-CM0pwYGaK-"}},{"cell_type":"code","source":["def topic_modeling_per_sentiment(df, num_topics=5):\n","    df = df.reset_index(drop=True)  # Reset index to avoid duplicate index issues\n","    sentiments = df['predicted_label'].unique()\n","    topic_results = {}\n","\n","    for sentiment in sentiments:\n","        texts = df[df['predicted_label'] == sentiment]['text'].tolist()\n","        if len(texts) == 0:\n","            print(f\"No text data for sentiment: {sentiment}\")\n","            continue\n","        tokenized_texts = [text.split() for text in texts]\n","        if len(tokenized_texts) == 0:\n","            print(f\"No tokens generated for sentiment: {sentiment}\")\n","            continue\n","        dictionary = corpora.Dictionary(tokenized_texts)\n","        if len(dictionary) == 0:\n","            print(f\"Empty dictionary for sentiment: {sentiment}\")\n","            continue\n","        corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n","        if len(corpus) == 0:\n","            print(f\"Empty corpus for sentiment: {sentiment}\")\n","            continue\n","        lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n","        topics = lda_model.print_topics()\n","        topic_results[sentiment] = topics\n","\n","    return topic_results"],"metadata":{"id":"BqAqu0ozGb7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nd_topics = topic_modeling_per_sentiment(df_combined_nd)\n","print(\"ND Data Topics by Sentiment:\")\n","for sentiment, topics in nd_topics.items():\n","    print(f\"\\nSentiment: {sentiment}\")\n","    for topic in topics:\n","        print(topic)\n"],"metadata":{"id":"j13XNvuCGh_a"},"execution_count":null,"outputs":[]}]}